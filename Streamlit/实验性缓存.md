# streamlitå®éªŒæ€§ç¼“å­˜æ¥å£(primitives)

Experimental cache primitives

## Overview

æ¦‚è¿°

Streamlit's unique execution model is a part of what makes it a joy to use: your code executes from top to bottom like a simple script for every interaction. There's no need to think about models, views, controllers, or anything of the sort.

streamlitçš„ç‹¬ç‰¹çš„æ‰§è¡Œæ¨¡å¼è®©å®ƒåœ¨ä½¿ç”¨æ—¶çœ‹èµ·æ¥éå¸¸æç¬‘, æ¯æ¬¡äº¤äº’, ä»£ç ä¼šåƒä¸€ä¸ªæ™®é€šçš„è„šæœ¬ä¸€æ ·è¢«ä»å¤´åˆ°å°¾æ‰§è¡Œä¸€é.

Whenever your code re-executes, a decorator called [`@st.cache`](https://docs.streamlit.io/library/api-reference/performance/st.cache)â€”which is a powerful primitive for memoization and state storage capabilitiesâ€”provides a caching mechanism that allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.

ä¸ç®¡ä»€ä¹ˆæ—¶å€™, å½“ä½ çš„ä»£ç é‡æ–°æ‰§è¡Œæ—¶, è¿™ä¸ªåä¸º@st.cacheè£…é¥°å™¨, å…·æœ‰éå¸¸å¼ºå¤§çš„ç”¨äºå­˜å‚¨, çŠ¶æ€ä¿æŒèƒ½åŠ›, è¿™å¥—ç¼“å­˜æœºåˆ¶å…è®¸ä»£ç ç»´æŒé«˜æ•ˆåœ¨åŠ è½½æ•°æ®ä»ç½‘ç»œæˆ–è€…è®¡ç®—å¯†é›†å‹çš„æ•°æ®é›†.

However, we've found that [`@st.cache`](https://docs.streamlit.io/library/advanced-features/caching) is hard to use and not fast. You're either faced with cryptic errors like `InternalHashError` or `UnhashableTypeError`. Or you need to understand concepts like [`hash_funcs`](https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter) and [`allow_output_mutation`](https://docs.streamlit.io/library/advanced-features/caching#example-1-pass-a-database-connection-around)

ç„¶è€Œæˆ‘ä»¬å‘ç°`@st.cache`éå¸¸éš¾ç”¨, è€Œä¸”è¿è¡Œä¹Ÿä¸å¿«.  ä½ é€šå¸¸ä¼šç¢°åˆ°å¥‡æ€ªçš„é”™è¯¯, åƒ`InternalHashError`å’Œ`UnhashableTypeError`. åŒæ—¶ä½ è¿˜éœ€è¦è¿æ¥é¡¹`hash_funcs`å’Œ`allow_output_mutation`è¿™äº›æ¦‚å¿µ.

Our solutions include two new primitives: [**`st.experimental_memo`**](https://docs.streamlit.io/library/api-reference/performance/st.experimental_memo) and [**`st.experimental_singleton`**](https://docs.streamlit.io/library/api-reference/performance/st.experimental_singleton). They're conceptually simpler and much, much faster. In some of our internal tests on caching large dataframes, `@st.experimental_memo` has outperformed `@st.cache` by an order of magnitude. That's over 10X faster! ğŸš€

æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸¤å¥—æœºåˆ¶, [**`st.experimental_memo`**](https://docs.streamlit.io/library/api-reference/performance/st.experimental_memo) and [**`st.experimental_singleton`**](https://docs.streamlit.io/library/api-reference/performance/st.experimental_singleton), äºŒè€…åœ¨æ¦‚å¿µä¸Šæ›´æ˜“äºç†è§£å’Œè¿è¡Œé€Ÿåº¦æ›´å¿«. åœ¨ä¸€äº›å†…éƒ¨æµ‹è¯•ä¸­, æ–°çš„æœºåˆ¶æ¯”`@st.cache`å¿«ä¸Šæ•°åå€, åœ¨åŠ è½½å¤§å‹æ•°æ®é›†æ—¶, `@st.experimental_memo` æ¯” `@st.cache`å¿«ä¸Šä¸€ä¸ªæ•°é‡çº§.

Let's take a look at the use-cases these *two* experimental APIs serve, and how they're a significant improvement over `@st.cache`.

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸¤ä¸ªå®éªŒæ€§APIæ¥å£, çœ‹è¿™ä¸¤ä¸ªæ¥å£å¦‚ä½•ç›¸æ¯”äº@st.cacheæœ‰å·¨å¤§çš„æ€§èƒ½æå‡.

*primitives: æ˜¯æŒ‡ç”±è‹¥å¹²å¤šæœºå™¨æŒ‡ä»¤æ„æˆçš„å®ŒæˆæŸç§ç‰¹å®šåŠŸèƒ½çš„ä¸€æ®µç¨‹åº, å…·æœ‰ä¸å¯åˆ†å‰²æ€§; å³åŸè¯­çš„æ‰§è¡Œå¿…é¡»æ˜¯è¿ç»­çš„, åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¸å…è®¸è¢«ä¸­æ–­.*

## Problem

é—®é¢˜

`@st.cache` was serving the following use-cases:

1. Storing computation results given different kinds of inputs. In Computer Science literature, this is called [**memoization**](https://en.wikipedia.org/wiki/Memoization).
1. å­˜å‚¨ç»™å®šä¸åŒç±»å‹è¾“å…¥çš„è®¡ç®—ç»“æœ, è¿™åœ¨è®¡ç®—æœºæœ¯è¯­ä¸­è¢«ç§°ä¸ºè®°å¿†.
2. Initializing an object exactly once, and reusing that same instance on each rerun for the Streamlit server's lifetime. This is called the [**singleton pattern**](https://en.wikipedia.org/wiki/Singleton_pattern).
2. ä¸€æ—¦å¯¹è±¡åˆå§‹åŒ–, åœ¨streamlitæœåŠ¡ç”Ÿå‘½å‘¨æœŸé‡Œ, æ¯æ¬¡è¿”å›çš„éƒ½æ˜¯ç›¸åŒçš„å®ä¾‹, è¿™è¢«ç§°ä½œå•ä¾‹æ¨¡å¼.
3. Storing global state to be shared and modified across multiple Streamlit sessions (and, since Streamlit is threaded, you need to pay special attention to thread-safety).
3. å­˜å‚¨å…¨å±€çŠ¶æ€ç”¨äºåœ¨å¤šä¸ªä¼šè¯ä¸­åˆ†äº«.

As a result of `@st.cache` trying to cover too many use-cases under a single unified API, it's both slow and complex.

ç”±äº@st.cacheå°è¯•å¤„ç†ä¸åŒçš„æƒ…å†µéƒ½ä½¿ç”¨å•ä¸€ä¸åŒºåˆ†çš„API, è¿™ä½¿å¾—ä»£ç è¿è¡Œéå¸¸æ…¢å’Œå¤æ‚.

## Solution

è§£å†³æ–¹æ¡ˆ

While `@st.cache` tries to solve two very different problems simultaneously (caching data and sharing global singleton objects), these new primitives simplify things by dividing the problem across two different APIs. As a result, they are faster and simpler.

@st.cacheå°è¯•ç”¨äºåŒæ—¶è§£å†³ä¸¤ä¸ªé—®é¢˜, ç¼“å­˜æ•°æ®å’Œåˆ†äº«å…¨å±€å•ä¾‹å˜é‡. è¿™é‡Œæœ‰æ–°çš„æœºåˆ¶æ¥ç®€åŒ–è¿™äº›äº‹æƒ…, é€šè¿‡ä¸¤ä¸ªAPIå°†é—®é¢˜åˆ†åˆ«å¼€æ¥. ä½¿å¾—ä»£ç è¿è¡Œæ›´å¿«å’Œæ›´ç®€å•.

### `@st.experimental_memo`

Use [`@st.experimental_memo`](https://docs.streamlit.io/library/api-reference/performance/st.experimental_memo) to store expensive computation which can be "cached" or "memoized" in the traditional sense. It has almost the exact same API as the existing `@st.cache`, so you can often blindly replace one for the other:

ä½¿ç”¨`@st.experimental_memo`ç”¨äºå­˜å‚¨è®¡ç®—å¯†é›†å†…å®¹, é€šå¸¸æ„ä¹‰ä¸Šå¯ä»¥è¢«è§†ä½œç¼“å­˜æˆ–è€…è®°å¿†. è¿™å‡ ä¹å’Œ`@st.cache`ä¸€æ ·çš„, åœ¨ä½¿ç”¨ä¸Šå¯ä»¥ç›´æ¥å–ä»£`@st,cache`å³å¯.

```python
import streamlit as st

@st.experimental_memo
def factorial(n):
    if n < 1:
        return 1
    return n * factorial(n - 1)

f10 = factorial(10)
f9 = factorial(9)  # Returns instantly!
```

#### Properties

å±æ€§

- Unlike `@st.cache`, this returns cached items by value, not by reference. This means that you no longer have to worry about accidentally mutating the items stored in the cache. Behind the scenes, this is done by using Python's `pickle()` function to serialize/deserialize cached values.
- ä¸åƒ`@st.cache`, `@st.experimental_memo`è¿”å›çš„æ˜¯å¯¹åº”é¡¹çš„å€¼è€Œä¸æ˜¯å¼•ç”¨. è¿™æ„å‘³ç€ä¸å†éœ€è¦æ‹…å¿ƒæ„å¤–ä¿®æ”¹äº†ç¼“å­˜ä¸­çš„å¯å˜å¯¹è±¡.  åœ¨è¿™èƒŒå, è¿™æ˜¯ç”±pythonçš„pickleå‡½æ•°è´Ÿè´£åºåˆ—åŒ–å’Œè§£åºåˆ—åŒ–ç¼“å­˜çš„å€¼.
- Although this uses a custom hashing solution for generating cache keys (like `@st.cache`), it does ***not*** use `hash_funcs` as an escape hatch for unhashable parameters. Instead, we allow you to ignore unhashable parameters (e.g. database connections) by prefixing them with an underscore.
- å°½ç®¡`st.experimental_memo`ä½¿ç”¨äº†è‡ªå®šä¹‰çš„`hash`è§£å†³æ–¹æ¡ˆç”¨äºäº§ç”Ÿç¼“å­˜é”®, ä½†æ˜¯å®ƒä¸ä½¿ç”¨`hash_funcs`æ¥è½¬ä¹‰æ¥å£(`hatch`)ç”¨äºå¤„ç†ä¸å¯å“ˆå¸Œçš„å‚æ•°. ç›¸åæˆ‘ä»¬å…è®¸ä½ å¿½è§†ä¸å¯å“ˆå¸Œçš„(`unhashable`)å‚æ•°é€šè¿‡å¢åŠ ä¸‹åˆ’çº¿å’Œå‰ç½®å‚æ•°, ä¾‹å¦‚æ•°æ®åº“è¿æ¥å®ä¾‹.

For example:

```python
import streamlit as st
import pandas as pd
from sqlalchemy.orm import sessionmaker

@st.experimental_memo
def get_page(_sessionmaker, page_size, page):
    """Retrieve rows from the RNA database, and cache them.

    Parameters
    ----------
    _sessionmaker : a SQLAlchemy session factory. Because this arg name is
                    prefixed with "_", it won't be hashed.
    page_size : the number of rows in a page of result
    page : the page number to retrieve

    Returns
    -------
    pandas.DataFrame
    A DataFrame containing the retrieved rows. Mutating it won't affect
    the cache.
    """
    with _sessionmaker() as session:
        query = (
            session
                .query(RNA.id, RNA.seq_short, RNA.seq_long, RNA.len, RNA.upi)
                .order_by(RNA.id)
                .offset(page_size * page)
                .limit(page_size)
        )

        return pd.read_sql(query.statement, query.session.bind)
```

### `@st.experimental_singleton`

[`@st.experimental_singleton`](https://docs.streamlit.io/library/api-reference/performance/st.experimental_singleton) is a key-value store that's shared across all sessions of a Streamlit app. It's great for storing heavyweight singleton objects across sessions (like TensorFlow/Torch/Keras sessions and/or database connections).

`@st.experimental_singleton`åœ¨æ•´ä¸ªä¼šè¯æœŸé—´ä»¥é”®å€¼æ–¹å¼å­˜å‚¨ç›¸å…³çš„å†…å®¹. è¿™å¯¹äºå­˜å‚¨é‡å‹(`heavyweight`)å•ä¾‹å¯¹è±¡éå¸¸æœ‰ç›Šåœ¨è·¨ä¼šè¯ä¸­(åƒTensorFlow, Torch..æˆ–è€…æ•°æ®åº“è¿æ¥).

Example usage:

æ¡ˆä¾‹

```python
import streamlit as st
from sqlalchemy.orm import sessionmaker

@st.experimental_singleton
def get_db_sessionmaker():
    # This is for illustration purposes only
    DB_URL = "your-db-url"
    engine = create_engine(DB_URL)
    return sessionmaker(engine)

dbsm = get_db_sessionmaker()
```

#### How this compares to `@st.cache`:

å¯¹æ¯”ä¸`@st.cache`

- Like `@st.cache`, **this returns items by reference.**
- å’Œ`@st.cache`ç›¸ä¼¼, éƒ½æ˜¯è¿”å›é¡¹çš„å¼•ç”¨
- You can return any object type, including objects that are not serializable.
- å¯ä»¥è¿”å›ä»»æ„å¯¹è±¡ç±»å‹, åŒ…æ‹¬æ— æ³•åºåˆ—åŒ–çš„å¯¹è±¡(***æ³¨æ„è¿™ä¸ªæ˜¯é‡ç‚¹***)
- Unlike `@st.cache`, this decorator does not have additional logic to check whether you are unexpectedly mutating the cached object. That logic was slow and produced confusing error messages. So, instead, we're hoping that by calling this decorator "singleton," we're nudging you to the correct behavior.
- ä¸åƒ`@st.cache`, `st.experimental_singleton`ä¸ä¼šæœ‰é¢å¤–é€»è¾‘ç”¨äºæ£€æµ‹æ„å¤–æ”¹å˜ç¼“å­˜çš„å¯¹è±¡. è¿™ä¸ªé€»è¾‘æ‰§è¡Œé€Ÿåº¦éå¸¸æ…¢åŒæ—¶è¿˜ä¼šäº§ç”Ÿä»¤äººå›°æƒ‘çš„é”™è¯¯ä¿¡æ¯. æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™ä¸ªç§°ä¸ºå•ä¾‹çš„è£…é¥°å™¨ç”¨äºçº æ­£ä½ æ­£ç¡®è¡Œä¸º.
- This does not follow the computation graph.
- è¿™ä¸è®¡ç®—å›¾ä¸ç¬¦(è¿™å¥è¯ä¸æ¸…æ¥šä½œè€…çš„æ„å›¾å…·ä½“è¡¨ç¤ºä»€ä¹ˆ).
- You don't have to worry about `hash_funcs`! Just prefix your arguments with an underscore to ignore them.
- ä¸å¿…è¦æ‹…å¿ƒ`hash_funcs`, åªè¦åœ¨ä½ çš„å‚æ•°å‰åŠ ä¸Šä¸‹åˆ’çº¿å°±å¯ä»¥å¿½ç•¥å®ƒä»¬.

#### Warning

è­¦å‘Š

Singleton objects can be used concurrently by every user connected to your app, and *you are responsible for ensuring that `@st.singleton` objects are thread-safe*. (Most objects you'd want to stick inside an `@st.singleton` annotation are probably already safeâ€”but you should verify this.)

å•ä¾‹æ¨¡å¼å¯¹è±¡å¯ä»¥è¢«è¿æ¥åˆ°appçš„ç”¨æˆ·åŒæ—¶ä½¿ç”¨. ä½ åº”è¯¥ç¡®ä¿ä½¿ç”¨å•ä¾‹è£…é¥°å™¨çš„å¯¹è±¡æ—¶çº¿ç¨‹å®‰å…¨çš„.(å¤§éƒ¨åˆ†çš„å¯¹è±¡, ä½ ä¸æƒ³æ”¾åˆ°è£…é¥°å™¨(å‡½æ•°)é‡Œé¢, è¿™äº›å¯¹è±¡å¯èƒ½æ˜¯å®‰å…¨çš„, ä½†æ˜¯ä½ åº”è¯¥éªŒè¯).

### Which to use: memo or singleton?

è¯¥ç”¨å“ªç§, `memo`è¿˜æ˜¯`singleton`?

Decide between `@st.experimental_memo` and `@st.experimental_singleton` based on your function's *return type*. Functions that return *data* should use `memo`. Functions that return *non-data objects* should use `singleton`.

ä½¿ç”¨`@st.experimental_memo`æˆ– `@st.experimental_singleton` , è¿™å–å†³äºä½ çš„å‡½æ•°è¿”å›çš„å†…å®¹çš„ç±»å‹. å½“è¿”å›æ•°æ®æ—¶, ä½¿ç”¨`memo`, è¿”å›çš„æ˜¯éæ•°æ®, è¿™ä½¿ç”¨å•ä¾‹`singleton`.

For example:

ä½¿ç”¨ç¤ºä¾‹

- Dataframe computation (pandas, numpy, etc): this is *dataâ€”*use `memo`
- pandas, numpyçš„dataframeè®¡ç®—æ•°æ®, memo
- Storing downloaded data: `memo`
- å­˜å‚¨å·²ä¸‹è½½çš„æ•°æ®, memo
- Calculating pi to n digits: `memo`
- è®¡ç®—nä½çš„piå€¼(åœ†å‘¨ç‡)
- Tensorflow session: this is a *non-data objectâ€”*use `singleton`
- TensorFlowä¼šè¯, è¿™æ˜¯éæ•°æ®å¯¹è±¡, ä½¿ç”¨å•ä¾‹.
- Database connection: `singleton`
- æ•°æ®åº“è¿æ¥(è¿™ä¸ªæ˜¯é‡ç‚¹), singleton(å•ä¾‹)

### Clear memo and singleton caches procedurally

ç¨‹åºåŒ–æ¸…é™¤è®°å¿†å’Œå•ä¾‹ç¼“å­˜

You can clear caches of functions decorated with `@st.experimental_memo` and `@st.experimental_singleton` *in code*. For example, you can do the following:

ä½ å¯ä»¥æ¸…é™¤`@st.experimental_memo` å’Œ `@st.experimental_singleton`è£…é¥°å™¨è£…é¥°çš„å‡½æ•°ç¼“å­˜åœ¨ä»£ç ä¸­. ä¾‹å¦‚:

```python
@st.experimental_memo
def square(x):
    return x**2

if st.button("Clear Square"):
    # Clear square's memoized values:
    square.clear()

if st.button("Clear All"):
    # Clear values from *all* memoized functions:
    st.experimental_memo.clear()
```

Pressing the "Clear Square" button will clear `square()`'s memoized values. Pressing the "Clear All" button will clear memoized values from all functions decorated with `@st.experimental_memo`.

æŒ‰ä¸‹æ¸…é™¤æŒ‰é’®å°†æ¸…é™¤`square()`å‡½æ•°ç¼“å­˜çš„å€¼. æŒ‰ä¸‹æ¸…é™¤å…¨éƒ¨çš„æŒ‰é’®å°†æ¸…é™¤æ‰æ‰€æœ‰ä½¿ç”¨@st.experimental_memoè£…é¥°å™¨çš„å‡½æ•°çš„ç¼“å­˜å€¼.

In summary:

æ€»ç»“:

- Any function annotated with `@st.experimental_memo` or `@st.experimental_singleton` gets its own `clear()` function automatically.
- æ‰€æœ‰@st.experimental_memo` or `@st.experimental_singleton`è£…é¥°çš„å‡½æ•°éƒ½å°†è‡ªåŠ¨è·å¾—clear()æ–¹æ³•.
- Additionally, you can use [`st.experimental_memo.clear()`](https://docs.streamlit.io/library/api-reference/performance/st.experimental_memo.clear) and [`st.experimental_singleton.clear()`](https://docs.streamlit.io/library/api-reference/performance/st.experimental_singleton.clear) to clear *all* memo and singleton caches, respectively.
- æ­¤å¤–ä¹Ÿå¯ä»¥ä½¿ç”¨[`st.experimental_memo.clear()`](https://docs.streamlit.io/library/api-reference/performance/st.experimental_memo.clear) and [`st.experimental_singleton.clear()`](https://docs.streamlit.io/library/api-reference/performance/st.experimental_singleton.clear)åˆ†åˆ«æ¸…é™¤æ‰€æœ‰è®°å¿†å’Œå•ä¾‹ç¼“å­˜.

#### Note

æ³¨æ„

The commands are **experimental**, so they're governed by our [experimental API process](https://docs.streamlit.io/library/advanced-features/prerelease#experimental).

è¿™äº›æ˜¯å®éªŒæ€§çš„, ä»–ä»¬å—åˆ°å®éªŒæ€§APIè¿›ç¨‹çš„ç®¡æ§.

These specialized **memoization** and **singleton** commands represent a big step in Streamlit's evolution, with the potential to *entirely replace* `@st.cache` at some point in 2022.

è¿™äº›é’ˆå¯¹æ€§çš„è®°å¿†(memoization)å’Œå•ä¾‹(singleton)å‘½ä»¤ä»£è¡¨ç€streamlitè¿›åŒ–çš„ä¸€å¤§æ­¥, å°†åœ¨2022å¹´å¯èƒ½å®Œå…¨æ›¿ä»£@st.cacheè£…é¥°å™¨åœ¨æŸäº›æ–¹é¢.

æ³¨: [memoization](https://zhuanlan.zhihu.com/p/56353893)

Yes, today you may use `@st.cache` for storing data you pulled in from a database connection (for a Tensorflow session, for caching the results of a long computation like changing the datetime values on a pandas dataframe, etc.). But these are very different things, so we made two new functions that will make it much faster! ğŸ’¨

å½“ç„¶, ä½ ç°åœ¨ä¾ç„¶è¿˜åœ¨ä½¿ç”¨@st.cacheè£…é¥°å™¨ç”¨äºå­˜å‚¨åŠ è½½è‡ªè¯¸å¦‚TensorFlow, æˆ–è€…å…¶ä»–éœ€è¦è€—è´¹æ—¶é—´è®¡ç®—å¾—åˆ°ç»“æœçš„æ•°æ®, ä½†æ˜¯è¿™é‡Œæœ‰äº†å¾ˆå¤§çš„ä¸åŒ, è¿™ä¸¤ç§ä¸åŒçš„åŠŸèƒ½å°†è®©ä»£ç è¿è¡Œæ›´å¿«.

Please help us out by testing these commands in real apps and leaving comments in [the Streamlit forums](https://discuss.streamlit.io/).